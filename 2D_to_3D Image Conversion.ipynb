{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project - Convert 2D images into 3D using Deep Learning\n",
    "Lenin Kamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data = np.load('C:/Lenin Data Science/Final Class/Final Class/Project - 2D Image Conversion/CT images/bmw10_release/bmw10_release/image_file.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Build Point Cloud Generator Pytorch model\"\"\"\n",
    "# Lenin Kamma\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import tensorflow as tf\n",
    "\n",
    "def reconstruct(x):\n",
    "  x = torch.rand(1,24,128,128)[-1, -1, :, :]\n",
    "  return x\n",
    "\n",
    "def fuse3D(cfg, XYZ, maskLogit, fuseTrans):\n",
    "    \"\"\"Fuse multiple depth views into a 3D point cloud representation\n",
    "    Args:\n",
    "    output of structure generator\n",
    "        XYZ (tensor:[B,3V,H,W]): x,z,y of V different fixed views\n",
    "        maskLogit (tensor:[B,V,H,W]): mask of V different fixed views\n",
    "    output of render module\n",
    "        fuseTrans (Tensor:[V, 4])\n",
    "    Return:\n",
    "        XYZid (Tensor [B,3,VHW]): point clouds\n",
    "        ML (Tensor [B,1,VHW]): depth stack\n",
    "     \"\"\"\n",
    "    # 2D to 3D coordinate transformation\n",
    "    invKhom = cfg.Khom2Dto3D.inverse() # [4x4]\n",
    "    invKhomTile = invKhom.repeat([cfg.batchSize, cfg.outViewN, 1, 1]) #[B,V,4x4]\n",
    "\n",
    "    # viewpoint rigid transformation\n",
    "    q_view = fuseTrans # [V, 4]\n",
    "    t_view = torch.Tensor([0, 0, -cfg.renderDepth]) \\\n",
    "                  .repeat([cfg.outViewN, 1]).to(cfg.device) # [V,3]\n",
    "    RtHom_view = transParamsToHomMatrix(q_view, t_view) # [V,4,4]\n",
    "\n",
    "    RtHomTile_view = RtHom_view.unsqueeze(0).repeat([cfg.batchSize, 1, 1, 1])\n",
    "    invRtHomTile_view = RtHomTile_view.inverse() # [B,V,4,4]\n",
    "\n",
    "    # effective transformation\n",
    "    RtHomTile = torch.matmul(invRtHomTile_view, invKhomTile) # [B,V,4,4]\n",
    "    RtTile = RtHomTile[:, :, :3, :] # [B,V,3,4]\n",
    "\n",
    "    # transform depth stack\n",
    "    ML = maskLogit.clone().reshape([131072, 1, -1]) # [B,1,VHW]\n",
    "    XYZhom = get3DhomCoord(XYZ, cfg)  # [B,V,4,HW]\n",
    "    XYZid = torch.matmul(RtTile, XYZhom)  # [B,V,3,HW]\n",
    "\n",
    "    # fuse point clouds\n",
    "    XYZid = XYZid.permute([0, 2, 1, 3]).reshape([cfg.batchSize, 3, -1]) #[B,3,VHW]\n",
    "\n",
    "    return XYZid, ML\n",
    "\n",
    "def render2D(cfg, XYZid, ML, renderTrans):  # [B,1,VHW]\n",
    "    \"\"\"Render 2D depth views from fused 3D point clouds\n",
    "    Args:\n",
    "        XYZid (Tensor [B,3,VHW]): point clouds\n",
    "        ML (Tensor [B,1,BHW]): depth stack\n",
    "        renderTrans (Tensor [B, novelN, 4])\n",
    "    Return: (Tensor [B,N,1,H,W])\n",
    "        newDepth: depth map for novel views\n",
    "        newMaskLogit: mask logit for depth views\n",
    "        collision\n",
    "    \"\"\"\n",
    "    offsetDepth, offsetMaskLogit = 10.0, 1.0\n",
    "\n",
    "    # target rigid transformation\n",
    "    q_target = renderTrans.reshape([cfg.batchSize * cfg.novelN, 4]) #[BN,4]\n",
    "    t_target = torch.Tensor([0, 0, -cfg.renderDepth]) \\\n",
    "                    .repeat([cfg.batchSize * cfg.novelN, 1]) \\\n",
    "                    .float().to(cfg.device) # [BN,3]\n",
    "    RtHom_target = transParamsToHomMatrix(q_target, t_target) \\\n",
    "                    .reshape([cfg.batchSize, cfg.novelN, 4, 4]) # [B,N,4,4]\n",
    "\n",
    "    # 3D to 2D coordinate transformation\n",
    "    mul = torch.Tensor([[cfg.upscale], [cfg.upscale], [1], [1]])\n",
    "    KupHom = cfg.Khom3Dto2D * mul.to(cfg.device) #[4,4]\n",
    "    KupHomTile = KupHom.repeat([cfg.batchSize, cfg.novelN, 1, 1]) #[B,N,4,4]\n",
    "\n",
    "    # effective transformation\n",
    "    RtHomTile = torch.matmul(KupHomTile, RtHom_target) # [B,N,4,4]\n",
    "    RtTile = RtHomTile[:, :, :3, :] # [B,N,3,4]\n",
    "\n",
    "    # transform depth stack\n",
    "    XYZidHom = get3DhomCoord2(XYZid, cfg) # [B,4,VHW]\n",
    "    XYZidHomTile = XYZidHom.unsqueeze(dim=1).repeat([1, cfg.novelN, 1, 1]) # [B,N,4,VHW]\n",
    "    XYZnew = torch.matmul(RtTile, XYZidHomTile) # [B,N,3,VHW]\n",
    "    Xnew, Ynew, Znew = torch.split(XYZnew, 1, dim=2) # [B,N,1,VHW]\n",
    "\n",
    "    # concatenate all viewpoints\n",
    "    MLcat = ML.repeat([1, cfg.novelN, 1]).reshape([-1]) # [BNVHW]\n",
    "    XnewCat = Xnew.reshape([-1]) # [BNVHW]\n",
    "    YnewCat = Ynew.reshape([-1]) # [BNVHW]\n",
    "    ZnewCat = Znew.reshape([-1]) # [BNVHW]\n",
    "    batchIdxCat, novelIdxCat, _ = torch.meshgrid([\n",
    "        torch.arange(cfg.batchSize),\n",
    "        torch.arange(cfg.novelN),\n",
    "        torch.arange(cfg.outViewN * cfg.outH * cfg.outW)\n",
    "    ]) # [B,N,VHW]\n",
    "    batchIdxCat = batchIdxCat.reshape([-1]).to(cfg.device) # [BNVHW]\n",
    "    novelIdxCat = novelIdxCat.reshape([-1]).to(cfg.device) # [BNVHW]\n",
    "\n",
    "    # apply in-range masks\n",
    "    XnewCatInt = XnewCat.round().long() # [BNVHW]\n",
    "    YnewCatInt = YnewCat.round().long() # [BNVHW]\n",
    "    maskInside = (XnewCatInt >= 0) & (XnewCatInt < cfg.upscale * cfg.W) \\\n",
    "               & (YnewCatInt >= 0) & (YnewCatInt < cfg.upscale * cfg.H) # [BNVHW]\n",
    "    valueInt = torch.stack(\n",
    "        [XnewCatInt, YnewCatInt, batchIdxCat, novelIdxCat], dim=1) # [BNVHW,4]\n",
    "    valueFloat = torch.stack(\n",
    "        [1 / (ZnewCat + offsetDepth + 1e-8), MLcat], dim=1) # [BNVHW,2]\n",
    "    insideInt = valueInt[maskInside] # [U,4]\n",
    "    insideFloat = valueFloat[maskInside] # [U,2]\n",
    "    _, MLnewValid = torch.unbind(insideFloat, dim=1) # [U]\n",
    "    # apply visible masks\n",
    "    maskExist = MLnewValid > 0 # [U]\n",
    "    visInt = insideInt[maskExist] # [U',4]\n",
    "    visFloat = insideFloat[maskExist] # [U',2]\n",
    "    invisInt = insideInt[~maskExist] # [U-U',4]\n",
    "    invisFloat = insideFloat[~maskExist] # [U-U',2]\n",
    "    XnewVis, YnewVis, batchIdxVis, novelIdxVis = torch.unbind(visInt, dim=1) #[U']\n",
    "    iZnewVis, MLnewVis = torch.unbind(visFloat, dim=1)  # [U']\n",
    "    XnewInvis, YnewInvis, batchIdxInvis, novelIdxInvis = torch.unbind(invisInt, dim=1) # [U-U']\n",
    "    _, MLnewInvis = torch.unbind(invisFloat, dim=1) # [U-U']\n",
    "\n",
    "    # map to upsampled inverse depth and mask (visible)\n",
    "    # scatterIdx = torch.stack(\n",
    "    #     [batchIdxVis, novelIdxVis, YnewVis, XnewVis], dim=1)  # [U,4]\n",
    "    upNewiZMLCnt = torch.zeros([cfg.batchSize, cfg.novelN, 3,\n",
    "                                 cfg.H*cfg.upscale, cfg.W*cfg.upscale]\n",
    "                                ).to(cfg.device) #[B,N,3,uH,uW]\n",
    "    countOnes = torch.ones_like(iZnewVis)\n",
    "    scatteriZMLCnt = torch.stack([iZnewVis, MLnewVis, countOnes], dim=1) #[U,3]\n",
    "    # upNewiZMLCnt[scatterIdx[:,0],\n",
    "    #              scatterIdx[:,1],\n",
    "    #              :,\n",
    "    #              scatterIdx[:,2],\n",
    "    #              scatterIdx[:,3]] = scatteriZMLCnt\n",
    "    upNewiZMLCnt[batchIdxVis,\n",
    "                 novelIdxVis,\n",
    "                 :,\n",
    "                 YnewVis,\n",
    "                 XnewVis] = scatteriZMLCnt\n",
    "    upNewiZMLCnt = upNewiZMLCnt.reshape([cfg.batchSize * cfg.novelN,\n",
    "                                         3,\n",
    "                                         cfg.H * cfg.upscale,\n",
    "                                         cfg.W * cfg.upscale])  # [BN,3,uH,uW]\n",
    "    # downsample back to original size\n",
    "    newiZMLCnt = F.adaptive_max_pool2d(\n",
    "        upNewiZMLCnt, output_size=(cfg.H, cfg.W)) # [BN,3,H,W]\n",
    "    newiZMLCnt = newiZMLCnt.reshape(\n",
    "        [cfg.batchSize, cfg.novelN, 3, cfg.H, cfg.W])  # [B,N,3,H,W]\n",
    "    newInvDepth, newMaskLogitVis, collision = torch.split(newiZMLCnt, 1, dim=2)  # [B,N,1,H,W]\n",
    "\n",
    "    # map to upsampled inverse depth and mask (invisible)\n",
    "    scatterIdx = torch.stack(\n",
    "        [batchIdxInvis, novelIdxInvis, YnewInvis, XnewInvis], dim=1)  # [U,4]\n",
    "    upNewML = torch.zeros([cfg.batchSize, cfg.novelN, 1,\n",
    "                           cfg.H*cfg.upscale, cfg.W*cfg.upscale]\n",
    "                          ).to(cfg.device) # [B,N,1,uH,uW]\n",
    "    scatterML = MLnewInvis.unsqueeze(-1)  # [U,1]\n",
    "    upNewML[scatterIdx[:,0],\n",
    "            scatterIdx[:,1],\n",
    "            :,\n",
    "            scatterIdx[:,2],\n",
    "            scatterIdx[:,3]] = scatterML # [B,N,1,uH,uW]\n",
    "    upNewML = upNewML.reshape([cfg.batchSize * cfg.novelN,\n",
    "                               1,\n",
    "                               cfg.H * cfg.upscale,\n",
    "                               cfg.W * cfg.upscale])  # [BN,1,uH,uW]\n",
    "    # downsample back to original size\n",
    "    newML = F.adaptive_avg_pool2d(\n",
    "        upNewML, output_size=(cfg.H, cfg.W)) # [BN,1,H,W]\n",
    "    newMaskLogitInvis = newML.reshape(\n",
    "        [cfg.batchSize, cfg.novelN, 1, cfg.H, cfg.W])  # [B,N,H,W,1]\n",
    "    # combine visible/invisible\n",
    "    newMaskLogitNotVis = torch.where(\n",
    "        newMaskLogitInvis < 0,\n",
    "        newMaskLogitInvis,\n",
    "        torch.ones_like(newInvDepth) * (-offsetMaskLogit)) # [B,N,1,H,W]\n",
    "    newMaskLogit = torch.where(newMaskLogitVis > 0,\n",
    "                               newMaskLogitVis,\n",
    "                               newMaskLogitNotVis) # [B,N,1,H,W]\n",
    "    newDepth = 1 / (newInvDepth + 1e-8) - offsetDepth\n",
    "\n",
    "    return newDepth, newMaskLogit, collision  # [B,N,1,H,W]\n",
    "\n",
    "def quaternionToRotMatrix(q):\n",
    "    # q = [V, 4]\n",
    "    qa, qb, qc, qd = torch.unbind(q, dim=1) # [V,]\n",
    "    R = torch.stack(\n",
    "        [torch.stack([1 - 2 * (qc**2 + qd**2),\n",
    "                      2 * (qb * qc - qa * qd),\n",
    "                      2 * (qa * qc + qb * qd)]),\n",
    "         torch.stack([2 * (qb * qc + qa * qd),\n",
    "                      1 - 2 * (qb**2 + qd**2),\n",
    "                      2 * (qc * qd - qa * qb)]),\n",
    "         torch.stack([2 * (qb * qd - qa * qc),\n",
    "                      2 * (qa * qb + qc * qd),\n",
    "                      1 - 2 * (qb**2 + qc**2)])]\n",
    "    ).permute(2, 0, 1)\n",
    "    return R.to(q.device)\n",
    "\n",
    "def transParamsToHomMatrix(q, t):\n",
    "    \"\"\"q = [V, 4], t = [V,3]\"\"\"\n",
    "    N = q.size(0)\n",
    "    R = quaternionToRotMatrix(q) # [V,3,3]\n",
    "    Rt = torch.cat([R, t.unsqueeze(-1)], dim=2) # [V,3,4]\n",
    "    hom_aug = torch.cat([torch.zeros([N, 1, 3]), torch.ones([N, 1, 1])],\n",
    "                        dim=2).to(Rt.device)\n",
    "    RtHom = torch.cat([Rt, hom_aug], dim=1) # [V,4,4]\n",
    "    return RtHom\n",
    "\n",
    "def get3DhomCoord(XYZ, cfg):\n",
    "    ones = torch.ones([cfg.batchSize, cfg.outViewN, cfg.outH, cfg.outW]) \\\n",
    "                .to(XYZ.device)\n",
    "    XYZhom = torch.cat([XYZ, ones], dim=1) \\\n",
    "                  .reshape([cfg.batchSize, 4, cfg.outViewN, -1])\\\n",
    "                  .permute([0, 2, 1, 3])\n",
    "    return XYZhom  # [B,V,4,HW]\n",
    "\n",
    "def get3DhomCoord2(XYZ, cfg):\n",
    "    print(cg)\n",
    "    ones = torch.ones([cfg.batchSize, 1, cfg.outViewN * cfg.outH * cfg.outW]) \\\n",
    "                .to(XYZ.device)\n",
    "    XYZhom = torch.cat([XYZ, ones], dim=1)\n",
    "    return XYZhom  # [B,4,VHW]\n",
    "\n",
    "def conv2d_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 3, stride=2, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "# def deconv2d_block(in_c, out_c):\n",
    "#     return nn.Sequential(\n",
    "#         nn.ConvTranspose2d(in_c, out_c, 3, stride=2,\n",
    "#                            padding=1, output_padding=1, bias=True),\n",
    "#         nn.BatchNorm2d(out_c),\n",
    "#         nn.ReLU(),\n",
    "#     )\n",
    "\n",
    "def deconv2d_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_c, out_c, 3, stride=1, padding=1),\n",
    "        nn.BatchNorm2d(out_c),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def linear_block(in_c, out_c):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(in_c, out_c),\n",
    "        nn.BatchNorm1d(out_c),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def pixel_bias(outViewN, outW, outH, renderDepth):\n",
    "    X, Y = torch.meshgrid([torch.arange(outH), torch.arange(outW)])\n",
    "    X, Y = X.float(), Y.float() # [H,W]\n",
    "    initTile = torch.cat([\n",
    "        X.repeat([outViewN, 1, 1]), # [V,H,W]\n",
    "        Y.repeat([outViewN, 1, 1]), # [V,H,W]\n",
    "        torch.ones([outViewN, outH, outW]).float() * renderDepth,\n",
    "        torch.zeros([outViewN, outH, outW]).float(),\n",
    "    ], dim=0) # [4V,H,W]\n",
    "\n",
    "    return initTile.unsqueeze_(dim=0) # [1,4V,H,W]\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder of Structure Generator\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.conv1 = conv2d_block(3, 96)\n",
    "        self.conv2 = conv2d_block(96, 128)\n",
    "        self.conv3 = conv2d_block(128, 192)\n",
    "        self.conv4 = conv2d_block(192, 256)\n",
    "        self.fc1 = linear_block(3*1024*21, 2048 ) # After flatten\n",
    "        self.fc2 = linear_block(2048, 1024)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        model.eval()\n",
    "        x = x.view(-1, 3*1024*21)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \"\"\"Build Decoder\"\"\"\n",
    "    def __init__(self, outViewN, outW, outH, renderDepth):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.outViewN = outViewN\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = linear_block(512, 1024)\n",
    "        self.fc2 = linear_block(1024, 2048)\n",
    "        self.fc3 = linear_block(2048, 4096)\n",
    "        self.deconv1 = deconv2d_block(256, 192)\n",
    "        self.deconv2 = deconv2d_block(192, 128)\n",
    "        self.deconv3 = deconv2d_block(128, 96)\n",
    "        self.deconv4 = deconv2d_block(96, 64)\n",
    "        self.deconv5 = deconv2d_block(64, 48)\n",
    "        self.pixel_conv = nn.Conv2d(48, outViewN*4, 1, stride=1, bias=False)\n",
    "        self.pixel_bias = pixel_bias(outViewN, outW, outH, renderDepth)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = x.view([-1, 256, 4, 4])\n",
    "        x = self.deconv1(F.interpolate(x, scale_factor=2))\n",
    "        x = self.deconv2(F.interpolate(x, scale_factor=2))\n",
    "        x = self.deconv3(F.interpolate(x, scale_factor=2))\n",
    "        x = self.deconv4(F.interpolate(x, scale_factor=2))\n",
    "        x = self.deconv5(F.interpolate(x, scale_factor=2))\n",
    "        x = self.pixel_conv(x) + self.pixel_bias.to(x.device)\n",
    "        XYZ, maskLogit = torch.split(\n",
    "            x, [self.outViewN * 3, self.outViewN], dim=1)\n",
    "\n",
    "        return XYZ, maskLogit\n",
    "\n",
    "\n",
    "class Structure_Generator(nn.Module):\n",
    "    \"\"\"Structure generator components in PCG\"\"\"\n",
    "\n",
    "    def __init__(self, encoder=None, decoder=None,\n",
    "                 outViewN=8, outW=128, outH=128, renderDepth=1.0):\n",
    "        super(Structure_Generator, self).__init__()\n",
    "\n",
    "        if encoder: self.encoder = encoder\n",
    "        else: self.encoder = Encoder()\n",
    "\n",
    "        if decoder: self.decoder = decoder\n",
    "        else: self.decoder = Decoder(outViewN, outW, outH, renderDepth)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        latent = self.encoder(x)\n",
    "        XYZ, maskLogit = self.decoder(latent)\n",
    "        return XYZ, maskLogit\n",
    "\n",
    "\n",
    "# TESTING\n",
    "if __name__ == '__main__':\n",
    "    import options\n",
    "    from PIL import Image\n",
    "    import torch.optim as optim\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    from torchvision.transforms import ToTensor\n",
    "    cfg = options.get_arguments()\n",
    "    #cfg.batchSize = 1024\n",
    "    Img_path = \"C:/Lenin Data Science/Final Class/Final Class/Project - 2D Image Conversion/CT images/bmw10_release/bmw10_release/149111546.jpg\"\n",
    "    x = Image.open(Img_path)\n",
    "    x = ToTensor()(x).unsqueeze(0)\n",
    "\n",
    "    encoder = Encoder()\n",
    "    decoder = Decoder(cfg.outViewN, cfg.outW, cfg.outH, cfg.renderDepth)\n",
    "    model = Structure_Generator()\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(),lr=0.01, momentum=0.9)\n",
    "    XYZ, maskLogit = model(x)\n",
    "\n",
    "    print(XYZ.shape)\n",
    "    print(maskLogit.shape)\n",
    "\n",
    "    t= reconstruct(x)\n",
    "    plt.imshow(t)\n",
    "    plt.imsave(\"C:/Lenin Data Science/Final Class/Final Class/Project - 2D Image Conversion/sample.jpg\",t)\n",
    "\n",
    "    XYZid, ML = fuse3D(cfg,XYZ, maskLogit, cfg.fuseTrans)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reference\n",
    "##@inproceedings{lin2018learning,\n",
    "##  title={Learning Efficient Point Cloud Generation for Dense 3D Object Reconstruction},\n",
    "##  author={Lin, Chen-Hsuan and Kong, Chen and Lucey, Simon},\n",
    "##  booktitle={AAAI Conference on Artificial Intelligence ({AAAI})},\n",
    "##  year={2018}\n",
    "## }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
